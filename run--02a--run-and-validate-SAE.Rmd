
<!-- This .Rmd file is set up to run either independently by checking for 
the presence of necessary objects in the global environment and running 
related scripts if not, or also as a child to the "main-doc". For that latter
reason, comments and headings that are not relevant to a final report to
pubic audiences are suppressed or moved to code chunks that can optionally be 
`echo`ed as desired. -->

```{r}
# run scripts for necessary packages and objects as necessary
if (!"meanNA"        %in% objects()) source("settings--main.R", echo = FALSE)
if (!"my_state_abbr" %in% objects()) source("method--read-customization-file.R", echo = FALSE)
if (!"bin_age"       %in% objects()) source("method--general-helper-functions.R", echo = FALSE)
if (!"direct_est_tract"     %in% objects()) source_rmd("method--small-area-estimation-functions.Rmd", echo = FALSE)
```

```{r load data for SAE runs}
# Load prepped data
for (f in c("acs1", "acs5", "pop")) {
  load(file = glue("{output_path}{f}_data_{my_output_tag}.Rda"))
}
```

### Motivating target choices for the SAE

Our application Small Area Estimation (SAE) methods attempts to bring PUMA-level phenomena and represent them down to the Census tract level. The specific choice of categories $c=1,...,C$ which we observe children as falling into at the PUMA level, and which we hope to predict at the tract level, depend on a balance of community characteristics that are (1) important to have measured in smaller areas as of the base-line year (as opposed to having lagged measures); and (2) at a level of detail that can be effectively captured.

With respect to that first item, at the time of this draft we expect that family income-to-poverty status and work status both expect to be highly dynamic, given changes in hiring and labor force participation and high inflation. With respect to the second item, while we would ideally want to have detailed information about household composition, precise income-to-poverty status, and details of work, the more detailed we set our $C$ categories, the less precision we will have.

The figures below examine how evenly the ACS1 sample is divided among different category schemes.

```{r function to show distribution}
show_acs1_cat_density <- function(my_cat, my_subtitle) {
  ggplot(acs1_child,
         aes(x = get(my_cat))) +
    geom_bar() +
    facet_wrap(~ child_agegroup_combo) +
    labs(title = "Number of ACS1 Sample Observations by Category Scheme",
         subtitle = my_subtitle,
         x = "") +
    theme_minimal() + 
    theme(axis.text.x = element_text(angle = 90,
                                     hjust = 1))
}
```

Below we see the size of the ACS1 sample partitioned by income-to-poverty categories defined by "100%"s. This is reasonably even at lower levels.

```{r examine overall distribution of ACS1 sample for poverty - pov}
show_acs1_cat_density("fam_incpov_ratio_cat_by100", "Income-to-Poverty by 100%s")
```

Next, we examine effectively the same distribution, but with more detail below 100% of the FPL. While this cuts the ACS1 sample more thinly, it also would better differentiate circumstances for families that may be very important for predicting eligibility status in the nowcasting.

```{r examine overall distribution of ACS1 sample with detail below poverty - mix}
# This examines whether sample density is likely sufficient to characterize 
# population shares below poverty (broken into ranges of 0-50 and 50-100)
show_acs1_cat_density("fam_incpov_ratio_cat_mix", "Income-to-Poverty with Detail Below Poverty")
```

Below, we see the size of the sample partitioned jointly by household work eligibility for CCDF, and income-to-poverty bands that are somewhat larger and chosen to align with the CCDF income threshold of `r local_ccdf_incratio_base`% of FPL. Although there is no requirement in either the SAE or nowcasting methods to have this alignment, this specification will allow for later direct comparison of SAE estimates of CCDF eligibility for the base year `r base_year` and the nowcast estimates.

```{r examine overall distribution of ACS1 sample with detail below poverty}
# Here is the density tailored to the local ccdf threshold

show_acs1_cat_density("work_incpov_mix", 
                      glue("Combo of Family Work Eligibility for CCDF and Income-to-Poverty Aligned with the\nCCDF Threshold of {local_ccdf_incratio_base}% FPL"))

```

The figures below explore the amount of variation in each category across PUMAs. We perform this check to examine the extent to which there are gaps--specifically, exact zeroes--of children in given categories.

```{r function examine PUMA-level counts of each category}
show_acs1_cat_density_by_puma <- function(my_cat, my_subtitle) {
 ggplot(acs1_child,
       aes(x = PUMA)) +
  geom_bar() +
  facet_grid(get(my_cat) ~ child_agegroup_combo,
             scales = "free_y") +
  labs(title = "Number of ACS1 Sample Observations by PUMA, by Age Group and Category",
       subtitle = my_subtitle,
       caption = "(Note: scale of y-axis varies to show detail)",
       y = "Count of Observations in ACS1 Sample") +
  theme_minimal() + 
  theme(axis.text.x = element_text(angle = 90,
                                   hjust = 1))  
}
```


```{r examine PUMA-level counts for poverty with detail, fig.height = 8}
show_acs1_cat_density_by_puma("fam_incpov_ratio_cat_mix", "Income-to-Poverty Ratio, with Detail Below 100% FPL")
```

```{r examine PUMA-level counts for combo of work eligibility and poverty, fig.height = 12}
# /!\ Some of these categories are quite thin: either zero or very low counts.
# While that can be okay for the purposes of the SAE (and we do check our results below 
# for whether we can meaningfully capture variation), the counts here are arguably 
# too thin for us to consider inflating our tract-level SAE estimated counts to
# equal the corresponding PUMA-level counts, given how much sample variation there
# presumably would be.

show_acs1_cat_density_by_puma("work_incpov_mix", 
                                glue("Combo of Family Work Eligibility for CCDF and Income-to-Poverty Aligned with the\nCCDF Threshold of {local_ccdf_incratio_base}% FPL"))
```

### Examine pattern contrasts between ACS1 and ACS5 measures

ACS1 data are naturally less variable than ACS5 because they reflect averages up to the PUMA level. Thus, the red line below has "density" to the left of the blue peak, and a longer trail of high values at the right. Those reflect tracts with values of poverty that are respectively lower and higher than the overall range of PUMA-level values.

The orange line--representing PUMA-level aggregates of the ACS5 data--is closer to the blue in the overall distribution. However, the fact that the blue line is slightly shifted left captures the fact that PUMA-level poverty rates for young children in `r base_year` are slightly lower than those in prior years.

```{r compare the distribution of values}
ggplot() +
  geom_density(data = acs1_puma_stats,
               aes(x = pct_pov),
               color = "blue") +
  geom_density(data = acs5tract,
               aes(x = incpov_lt6_r0to100_est),
               color = "red") +
  geom_density(data = acs5puma,
               aes(x = incpov_lt6_r0to100_est),
               color = "orange") +
  scale_x_continuous(labels = percent) + 
  labs(title = "Comparing Distributions of Poverty Rates for Children Under 6",
       subtitle = glue("<span style='color:blue'>PUMA-level ACS1-year Data for {base_year}</span><br>",
                       "<span style='color:red'>Tract-level ACS5-year Data for {acs5_year-4}-{acs5_year}</span><br>",
                       "<span style='color:orange'>PUMA-level ACS5-year Data for {acs5_year-4}-{acs5_year}</span>"),
       # 
       # 
       # subtitle = glue("ACS1 data reflects PUMA-level values for {base_year}\n",
       #                 "ACS5 data reflects tract-level values for 2015-2019"),
       x = "Poverty Rate for Children Under 6",
       y = "Density of the Distribution") +
  theme_minimal() +
  theme(plot.subtitle = element_markdown(lineheight = 1.1))
```


<!-- Note: Previous versions of this code explored the proportions of sample
falling in combinations of other categorical buckets, including education or
race/ethnicity of the head of household, and the "vulnerability" of the HOH
industry of employment. Much of this was early in code development, and at times
surrounding the COVID-19 pandemic when things like socioeconomic status and 
industry of employment was particularly significant.

That descriptive exploration can be seen as of the following commit of this file
at:
https://github.com/chapinhall/elpep/blob/be808bdf9b99087b528580e510abc759c06a40a4/run--02a--run-and-validate-SAE.Rmd

-->

### Results of SAE Estimation

```{r setup run options based on local ccdf details}
# Categories of income-to-poverty ratios available in the ACS 5-year data
# "r0to50", "r50to74", "r75to99", "r100to124", "r125to149", "r150to174", "r175to184", 
# "r185to199", "r200to299", "r300to399", "r400to499", "r500plus"

# NSM -- Note: the `incpov_ranges_for_estimation`, which are predictors available
# in the ACS 5-year data, do not strictly need to line up to the `incpov_spec_field` 
# categories, which is constructed in ACS 1-year data. Strictly speaking, they just
# need to be reasonable as predictors. However, because they are preserved in
# the output, and since the ACS1 categories are how the SAE estimates are divided,
# it's useful to build comparisons if there is alignment.

incpov_spec_field_mix <- "fam_incpov_ratio_cat_mix"   
incpov_spec_field_100 <- "fam_incpov_ratio_cat_by100"
incpov_ref_value  <- "0%-50%"  # "0%-100%"
incpov_ranges_for_estimation <- c("0to50", "50to100", "100to199", "200to299")

# For the sake of building comparisons of SAE estimates, and "now"-cast estimates, 
# the specification of work_incpov is aligned with the local CCDF threshold. Thus,
# the final now-cast CCDF eligibility status--which does not strictly need to 
# align with even the SAE estimates--can be compared to AcS 1-year or SAE 
# estimates by adding up work_incpov_status75 %in% c("work). Thus, if the 
# CCDF threshold is 225 FPL, the SAE estimates `work_incpov_status75 %in% 
# c("WorkElig_0%-75%", "WorkElig_75%-150%", "WorkElig_150%-225%")` could be
# added to make a direct comparison.

# Our current specification of `work_incpov_mix` embeds this alignment 
# by adding the income threshold directly, in addition to the 100% for Head Start

work_incpov_spec_field <- "work_incpov_mix"

```


```{r set specifications for sae}
# /!\ Consider adding race/ethnicity and industry of workers
sae_controls_simple_05 <- 
  paste0("incpov_lt6_r", incpov_ranges_for_estimation, "_est")

sae_controls_simple_612 <- 
  paste0("incpov_6to11_r", incpov_ranges_for_estimation, "_est")

sae_extra_controls <- 
  c(paste0(c("f_lesshs", "m_lesshs", "f_hsgrad", "m_hsgrad", "f_somecoll", "m_somecoll", # This is omitting `f_coll` and `m_coll`
             "pctMale_noSp", "pctFemale_noSp",                                           # This is omitting `pctMarried`
             "employrate_m_a2534", "employrate_f_a2534", "lfrate_m_a2534", "lfrate_f_a2534"), 
           "_est"))

sae_controls_added_2534_05 <- 
  c(sae_controls_simple_05, sae_extra_controls)

sae_controls_added_2534_612 <- 
  c(sae_controls_simple_612, sae_extra_controls)
```

```{r subset data for the SAE}
acs1_child[j = PUMA := str_pad(PUMA, width = 5, side = "left", pad = "0")]
stopifnot(all(geo_crosswalk$PUMA %in% acs1_child$PUMA))

acs1_child05_sae <- 
  acs1_child[between(AGEP, 0, 5)]

acs1_child612_sae <- 
  acs1_child[between(AGEP, 6, 12)]

acs5tract_sae <- acs5tract

# If a specific county has been specified, subset data to it
if (exists("my_county_fip")) {
  acs1_child05_sae <- 
    acs1_child05_sae %>% 
    .[j = in_my_geo := 1*(PUMA %in% geo_crosswalk[COUNTYFIP == my_county_fip]$PUMA)] %>% 
    .[in_my_geo == 1]
  
  acs1_child612_sae <- 
    acs1_child612_sae %>% 
    .[j = in_my_geo := 1*(PUMA %in% geo_crosswalk[COUNTYFIP == my_county_fip]$PUMA)] %>% 
    .[in_my_geo == 1]
  
  acs5tract_sae <- 
    acs5tract_sae %>% 
    .[COUNTYFIP == my_county_fip]
}

```

```{r implement small acs5year cleanup}
# /!\ Consider moving this to the `1d` script
# Replace NAs of 25-34 year old employment stats with equivalent statistics across ages
a2534_vars <- str_subset(cn(acs5tract_sae), "2534")
acs5tract_sae[j = c(a2534_vars) := 
                 lapply(a2534_vars, function(x) ifelse(is.na(get(x)), get(str_replace(x, "_a2534", "")), get(x)))]
```

```{r bring in base year population estimates to replace those in the acs 5-year}
# Select specific age ranges of interest
pop_by_age_base <- 
  pop_by_age %>% 
  filter(year == base_year) %>% 
  # Rename population values to avoid conflict with equivalent ACS5 counts, which
  # are for the prior 5 year period, not estimated for the base year as these are
  select(GEOID = GEOID_TR20, 
          age_0to5_count_base =  age_0to5_count, 
         age_6to12_count_base = age_6to12_count)

# Merge to the ACS5-year data
acs5tract_sae_counts <- 
  acs5tract_sae %>% 
  merge(pop_by_age_base,
        by = "GEOID",
        all.x = TRUE) %>% 
  # /!\ These are placeholder values, which of course improperly assume that the
  # base-year estimates are perfectly precise. This must be replaced when possible.
  mutate( age_0to5_count_se_base = 0,
         age_6to12_count_se_base = 0)

# Compare ACS and 2020 counts
ggplot(acs5tract_sae_counts,
       aes(x = age_0to5_count,
           y = age_0to5_count_base)) +
  geom_point() + 
  geom_abline(color = "red") +
  labs(title = "Comparison of Tract-level Counts of Children Aged 0-5",
       x = "ACS 5-Year",
       y = glue("Estimate for Base Year ({base_year})")) +
  theme_minimal()
  # /!\ There are some highly off-axis values. Check these out, and/or convert to
  # ggplotly to allow for inspection
```

```{r establish function to run SAE method}
run_saes_byCtrls <- function(vul_vars, sae_input_data, my_controls) {
  
  ### Determine which age count variables should used
  # Note: there are other ways to check against the second argument, such as
  # deparse(substitute(sae_input_data)) to get the character input from the
  # function call, but that approach wouldn't work from the parallelized 
  # function calls
  if (identical(sae_input_data, acs1_child05_sae)) {
    pop_n_var    <- "age_0to5_count_base"    # "age_0to5_count" would be the alternative, from ACS5 data
    pop_n_se_var <- "age_0to5_count_se_base" # "age_0to5_se"    would be the alternative, from ACS5 data
  } else if (identical(sae_input_data, acs1_child612_sae)) {
    pop_n_var    <- "age_6to12_count_base"    # "age_6to12_count" would be the alternative, from ACS5 data
    pop_n_se_var <- "age_6to12_count_se_base" # "age_6to12_se"    would be the alternative, from ACS5 data
  }
  #browser() 
  run_sae_combos(vul_vars       = vul_vars,
                 sae_input_data = sae_input_data,
                 sae_controls   = my_controls,
                 aux_data       = acs5tract_sae_counts,
                 aux_n_var      = pop_n_var,
                 aux_n_se_var   = pop_n_se_var,
                 verbose = TRUE)
}
```


```{r run SAE method}

parallelize_sae <- TRUE

if (parallelize_sae) {
  
  runlist <- 
    list(sae_results_incpov_simple_05     = list(incpov_spec_field_mix,  acs1_child05_sae,  sae_controls_simple_05),
         sae_results_incpov_simple_612    = list(incpov_spec_field_mix,  acs1_child612_sae, sae_controls_simple_612),
         sae_results_incpov_ctrls_mix_05  = list(incpov_spec_field_mix,  acs1_child05_sae,  sae_controls_added_2534_05),
         sae_results_incpov_ctrls_mix_612 = list(incpov_spec_field_mix,  acs1_child612_sae, sae_controls_added_2534_612),
         sae_results_incpov_ctrls_100_05  = list(incpov_spec_field_100,  acs1_child05_sae,  sae_controls_added_2534_05),
         sae_results_incpov_ctrls_100_612 = list(incpov_spec_field_100,  acs1_child612_sae, sae_controls_added_2534_612),
         sae_results_ccdf_wk_pov_05       = list(work_incpov_spec_field, acs1_child05_sae,  sae_controls_added_2534_05),
         sae_results_ccdf_wk_pov_612      = list(work_incpov_spec_field, acs1_child612_sae, sae_controls_added_2534_612),
         sae_results_ccdf_wk_sp_05        = list("work_spouse_status",   acs1_child05_sae,  sae_controls_added_2534_05))
  
  # Code to recruit (almost) as many core processors as are available (but not more than we need)
  cores <- detectCores() - 1 # not to overload your computer
  cores <- min(cores, length(runlist))
  cl <- makeCluster(cores) 
  registerDoParallel(cl)
  
  sae_results <- foreach (r = seq_along(runlist)) %dopar% {
    source("settings--main.R")
    
    run_saes_byCtrls(runlist[[r]][[1]], 
                     runlist[[r]][[2]],
                     runlist[[r]][[3]])
  }
  stopCluster(cl)
  
  # Separate parallelized results to conform to downstream code
  # /!\ Note: arguably, it'd be wise to update the downstream code to conform
  # to the list format output by parallelization
  for (r in seq_along(sae_results)) {
    assign(names(runlist)[r],
           sae_results[[r]])
  }
  
} else {
  sae_results_incpov_simple_05     <- run_saes_byCtrls(incpov_spec_field_mix,  acs1_child05_sae,  sae_controls_simple_05)
  sae_results_incpov_simple_612    <- run_saes_byCtrls(incpov_spec_field_mix,  acs1_child612_sae, sae_controls_simple_612)
  sae_results_incpov_ctrls_mix_05  <- run_saes_byCtrls(incpov_spec_field_mix,  acs1_child05_sae,  sae_controls_added_2534_05)
  sae_results_incpov_ctrls_mix_612 <- run_saes_byCtrls(incpov_spec_field_mix,  acs1_child612_sae, sae_controls_added_2534_612)
  sae_results_incpov_ctrls_100_05  <- run_saes_byCtrls(incpov_spec_field_100,  acs1_child05_sae,  sae_controls_added_2534_05)
  sae_results_incpov_ctrls_100_612 <- run_saes_byCtrls(incpov_spec_field_100,  acs1_child612_sae, sae_controls_added_2534_612)
  sae_results_ccdf_wk_pov_05       <- run_saes_byCtrls(work_incpov_spec_field, acs1_child05_sae,  sae_controls_added_2534_05)
  sae_results_ccdf_wk_pov_12       <- run_saes_byCtrls(work_incpov_spec_field, acs1_child612_sae, sae_controls_added_2534_612)
  sae_results_ccdf_wk_sp_05        <- run_saes_byCtrls("work_spouse_status",   acs1_child05_sae,  sae_controls_added_2534_05)
}

specs <- c("incpov_simple_05", 
           "incpov_simple_612",
           "incpov_ctrls_mix_05",
           "incpov_ctrls_mix_612",
           "incpov_ctrls_100_05",
           "incpov_ctrls_100_612",
           "ccdf_wk_sp_05",
           "ccdf_wk_pov_05",
           "ccdf_wk_pov_612") 

```

```{r}
# Organize contents
sae_aux_data_acs5 <- acs5tract_sae
sae_controls_list <- list(sae_controls_simple_05,
                          sae_controls_simple_612,
                          sae_controls_added_2534_05,
                          sae_controls_added_2534_612)
results <- paste0("sae_results_", specs)

# Save the bundle of objects
save(list = c(results, "specs", "incpov_ref_value", "sae_controls_list", "sae_aux_data_acs5"),
     file = glue("{output_path}sae_sensitivity_estimates_interim_{my_output_tag}.Rda"))

if (FALSE) {
  load(file = glue("{output_path}sae_sensitivity_estimates_interim_{my_output_tag}.Rda")) 
}
```


#### Compare Direct versus FH Output Estimates

As a reality check, confirm that a single "direct" estimate applies to all tracts within each given PUMA. The result should be that there are all "0" numbers of duplicates. 

```{r check that all tracts in each puma have the same direct estimate, eval = developer_mode}
sae_results_incpov_simple_05$sae_out %>% 
  group_by(vc_value, GEOID, PUMA) %>% 
  summarize(ndups = n_distinct(share_direct) - 1) %>% 
  with(table(ndups))
```

Here, we are looking to see generally how the "Fay-Herriot" method results, which are detailed down to the tract level, compared to the direct estimates, which are carried down from PUMA-level ACS 1-year estimates.

```{r compare direct vs FH output - first spec}
sae_results_incpov_simple_05[["comp_viz"]] +
  labs(x = "Direct Share Estimate",
       y = "Fay-Herriott Share Estimates (Simple Spec)") +
  theme(legend.position = "none")
```

```{r compare direct vs FH output - second spec}
sae_results_incpov_ctrls_mix_05[["comp_viz"]] +
  labs(x = "Direct Share Estimate",
       y = "Fay-Herriott Share Estimates (Full Spec)") +
  theme(legend.position = "none")
```

```{r examine share of reliance on direct versus model}
# Check our understanding of the fh() weight calculation
display_lambda_wgt <- 
  sae_results_incpov_ctrls_mix_05$sae_out %>% 
  mutate(lambda_wgt_direct  = share_model_se^2 / (share_model_se^2 + share_direct_se^2),
         implied_wgt_direct = (share_fh - share_model) / (share_direct - share_model))
# %>% 
#   merge(geo_crosswalk[j = .(GEOID, aux_geo_label)] %>% unique(),
#         by = "GEOID", 
#         all.x = TRUE)

stopifnot(all(display_lambda_wgt[j = quantile(round(lambda_wgt_direct, 5) - round(implied_wgt_direct, 5),
                                              na.rm = TRUE) == 0]))

display_lambda_wgt_pov <- 
  display_lambda_wgt %>% 
  filter(vc_value == incpov_ref_value)

ggplot(display_lambda_wgt_pov,
       aes(x = reorder(GEOID, lambda_wgt_direct),
           y = lambda_wgt_direct) # , color = aux_geo_label == "Cook") ... this would allow for coloring a given focal region
       ) + 
  geom_point() +
  labs(title = case_when(my_output_tag == "IL" ~ "For Share Below Poverty, Weight on 'Direct', PUMA-Derived Estimates\nis Typically Zero",
                         TRUE ~ ""),
       x = "Tracts, Ordered by Lambda",
       y = "% Weight Placed on Direct Estimate") +
  scale_y_continuous(labels = percent,
                     limits = c(0, 1.0)) +
  # scale_color_manual(breaks = c(FALSE, TRUE),
  #                    values = c("gray", "lightblue"),
  #                    labels = c("Non-Cook", "Cook")) +
  theme(axis.text.x = element_blank())

ggplot(display_lambda_wgt_pov,
       aes(x = reorder(GEOID, share_direct))) +
  geom_point(aes(y = share_direct),
             color = "lightblue") +
  geom_point(aes(y = share_model),
             color = "orange")

ggplot(display_lambda_wgt,
       aes(x = share_direct,
           y = share_model)) +
  geom_point(alpha = 0.1) +
  geom_abline(color = "blue") +
  facet_wrap(~ vc_value) +
  labs(title = "Comparison of SAE Output -- Model-Estimated Share (y) vs Direct (x)") +
  theme_minimal()
  
```

How much do discrepancies between model and direct estimates exist across PUMAs?

```{r}
plot_by_ordered_puma <- 
  sae_results_incpov_ctrls_mix_05$sae_out %>% 
  as.data.table() %>% 
  .[j = rank := rank(share_direct, ties.method = "random"), 
    by = vc_value] %>% 
  .[j = puma_by_rank := factor(PUMA, levels = unique(PUMA[order(rank)])),
    by = vc_value]

ggplot(plot_by_ordered_puma,
       aes(x = puma_by_rank)) +
    geom_point(aes(y = share_model),
               color = "red",
               alpha = 0.1) +
    geom_point(aes(y = share_direct),
               color = "blue",
               alpha = 0.1) +
    facet_wrap(~ vc_value,
               scales = "free") +
    theme_minimal()  +
    theme(axis.text.x = element_text(angle = 90)) +
  labs(title = 
         str_wrap(case_when(my_output_tag == "Cook2022" ~ "Some PUMAs have Direct Estimates that Lie Significantly Outside of the Range of Model Estimates")))
```

Identify which PUMAs have the greatest discrepancies between direct and model estimates, especially for lower levels of income.

```{r}
sae_results_incpov_ctrls_mix_05$sae_out %>% 
  group_by(vc_value, PUMA) %>% 
  summarize(model_vs_direct_var = (sum((share_model - share_direct)^2)/n()) %>% round(3),
            model_vs_direct_above = mean(share_model > share_direct) %>% percent(),
            model_vs_direct_oneside = mean((share_model > share_direct)*(median(share_model) > share_direct)) %>% percent()) %>% 
              # This checks for % of tracts on the same side as the median) 
  filter(vc_value == "0%-50%") %>% 
  datatable()

```

```{r examine differences in direct and model by geography}
if (my_output_tag == "Cook2022") {
  crosswalk_cpt <- 
    read.csv(glue("H:/DFSS Community Assessment/Community Assessment/2020-2021 project year/program eligibility estimation/input/PUMA_Tract_CCA_equivalency2010.csv")) %>% 
    data.table() %>% 
    rename(COUNTYFIP = county, 
           PUMA      = puma,
           CCA       = cca,
           TRACTFIP  = tract) %>% 
    mutate(TRACTFIP = str_pad(TRACTFIP, width = 6, side = "left", pad = "0"),
           GEOID = paste0("17031", TRACTFIP))

  aug_sae_out <- 
    sae_results_incpov_ctrls_mix_05_out %>% 
    merge(crosswalk_cpt %>% select(GEOID, CCA) %>% unique(),
          by = "GEOID",
          all.x = TRUE) 
  
  # Plot by PUMA on x-axis
  ggplot(aug_sae_out %>% .[order(share_direct)] %>% .[j = rank := 1:.N] %>% .[!is.na(CCA)],
         aes(x = factor(PUMA))) +
    geom_point(data = aug_sae_out[CCA != 54],
               aes(y = share_model),
               color = "red",
               alpha = 0.1) + # CCA == 54 is Riverdale
    geom_point(data = aug_sae_out[CCA == 54],
               aes(y = share_model),
               color = "black") + # CCA == 54 is Riverdale
    geom_point(aes(y = share_direct),
               color = "blue",
               alpha = 0.1) +
    facet_wrap(~ vc_value) +
    # scale_color_manual(breaks = c(F, T),
    #                    values = c("red", "black")) +
    theme_minimal()  +
    theme(axis.text.x = element_text(angle = 90))
  
}
```


#### Examine the Estimated Composition of Tracts

```{r examine SAE share composition by tract -- FH estimates}
plot_sae_comp_by_tract <- function(sae_est, base_cat, title) {
  
  tract_order <- sae_est[vc_value == base_cat] %>% as.data.table() %>% .[order(-share_fh_trunc)] %>%  .[j = rank := 1:.N]
  
  ggplot(sae_est %>% merge(tract_order[, .(GEOID, rank)], by = "GEOID"),
         aes(x = reorder(GEOID, rank),
             y = share_fh_trunc,
             color = vc_value)) +
    geom_point(alpha = 0.3) +
    geom_hline(yintercept = 0) +
    scale_y_continuous(labels = percent) +
    scale_color_discrete(name = "Vulnerability Category") +
    labs(title = title,
         x = glue("Tracts, in order of pop'n share in {base_cat}"),
         y = "SAE estimated share") +
    theme(axis.text.x = element_blank()) 
}
```

<!-- Below, the smoothness of the "simple" method suggests ... -->

```{r}
# /!\ Consider ways to display them in a grid.
# /!\ Consider how to loop or sapply this, to be more compact
plot_sae_comp_by_tract(sae_results_incpov_simple_05$sae_out,  
                       base_cat = incpov_ref_value,
                       title = "Vulnerability Cat Composition by Tract -- Simple Spec")
plot_sae_comp_by_tract(sae_results_incpov_ctrls_mix_05$sae_out,   
                       base_cat = incpov_ref_value, 
                       title = "Vulnerability Cat Composition by Tract -- Added Controls (2534)")

plot_sae_comp_by_tract(sae_results_ccdf_wk_sp_05$sae_out,   
                       base_cat = "WorkElig_SpousePresent", 
                       title = "Vulnerability Cat Composition by Tract -- Work Eligibility by Presence of Spouse")

plot_sae_comp_by_tract(sae_results_ccdf_wk_pov_05$sae_out,   
                       base_cat = str_subset(sae_results_ccdf_wk_pov_05$sae_out$vc_value, "^WorkElig_0%") %>% unique(), 
                       title = "Vulnerability Cat Composition by Tract -- Work Eligibility by Income-to-Pov")
```

#### Examine Aggregate Comparison Plots

Because PUMA data are very thin, we will not adjust our tract-level estimates to match their totals. Instead, we will just compare them to get a rough sense of correspondence.

However, it seems that the addition of detailed controls has improved alignment of the estimated measures with the ACS.

```{r compare sums of estimates to PUMA-level with observed PUMA values -- simple vs extended controls, results = "asis"}
# for (spec in specs) {
#   get(glue("sae_results_{spec}"))[["agg_comp"]]$agg_comp_plot %>% 
#     print()
# }

if (str_detect(my_output_tag, "Cook2023")) {
  cat("Controls set makes a small difference in PUMA-level aggregate comparisons.")
}

sae_results_incpov_simple_05[["agg_comp_fh"]]$agg_comp_plot +
  labs(title = "PUMA-level SAE Estimates vs ACS1 Counts",
       subtitle = "Inc-to-Pov, Simple Ctrls, FH-based Est, Ages 0-5",
       x = "Observed ACS 1-year Counts") 

sae_results_incpov_ctrls_mix_05[["agg_comp_fh"]]$agg_comp_plot +
  labs(title = "PUMA-level SAE Estimates vs ACS1 Counts",
       subtitle = "Inc-to-Pov, Extended Ctrls, FH-based Est, Ages 0-5",
       x = "Observed ACS 1-year Counts") 

```


```{r compare sums of estimates to PUMA-level with observed PUMA values -- FH versus model}
if (str_detect(my_output_tag, "Cook2023")) {
  cat("Use of model-based versus full Fay-Herriott estimates makes a difference, and are arguably worse at fitting PUMA-level aggregates. That's expected, since that the FH estimates use shrinkage to bring estimates closer to PUMA-level averages. Also, use of the model-based estimates reflects a tradeoff of respecting spatial heterogeneity as genuine rather than as outliers.")
}

sae_results_incpov_ctrls_mix_05[["agg_comp_fh"]]$agg_comp_plot +
  labs(title = "PUMA-level SAE Estimates vs ACS1 Counts",
       subtitle = "Inc-to-Pov, Extended Ctrls, FH-based Est, Ages 0-5",
       x = "Observed ACS 1-year Counts")

sae_results_incpov_ctrls_mix_05[["agg_comp_model"]]$agg_comp_plot +
  labs(title = "PUMA-level SAE Estimates vs ACS1 Counts",
       subtitle = "Inc-to-Pov, Extended Ctrls, Model-based Est, Ages 0-5",
       x = "Observed ACS 1-year Counts")
```


```{r compare sums of estimates to PUMA-level with observed PUMA values -- incpov mix vs by 100s}
if (str_detect(my_output_tag, "Cook2023")) {
  cat("The mixed-interval estimates show that there are significant under-estimates of 50-100% FPL counts. This is perhaps unsurprising given that 50-100% is a relatively small target. Although it is as 'wide' as 0-50%, it does not have the density of households with very little actual or formal income that are near 0. Although this specification gives us an accurate (in aggregates) estimation of 0-50% FPL, the underestimate of 50-100% will cause underestimates of aggregate counts for any larger interval that includes them: <100% FPL, <200% FPL, etc., both as an input to the now-casting, as well as direct estimates of baseline year eligibility.\n\n The good news is that a specification of by-100s intervals is useful for getting 0-100% FPL counts right. The bottom line is that we can use a combination of these specifications as part of our now-casting and baseline calculations.")
}

sae_results_incpov_ctrls_mix_05[["agg_comp_fh"]]$agg_comp_plot +
  labs(title = "PUMA-level SAE Estimates vs ACS1 Counts",
       subtitle = "Inc-to-Pov (mixed intervals), Extended Ctrls, FH-based Est, Ages 0-5",
       x = "Observed ACS 1-year Counts")

sae_results_incpov_ctrls_100_05[["agg_comp_fh"]]$agg_comp_plot +
  labs(title = "PUMA-level SAE Estimates vs ACS1 Counts",
       subtitle = "Inc-to-Pov (by 100s), Extended Ctrls, FH-based Est, Ages 0-5",
       x = "Observed ACS 1-year Counts")
```


```{r compare sums of estimates to PUMA-level with observed PUMA values -- 0-5 to 6-12}
if (str_detect(my_output_tag, "Cook2023")) {
  cat("The 6-12 aggregates seems to have more noise than 0-5, although they are more zoomed-in on smaller counts, and they seem to notably avoid the under-estimation of 50-100% FPL counts.")
}

sae_results_incpov_ctrls_mix_05[["agg_comp_fh"]]$agg_comp_plot +
  labs(title = "PUMA-level SAE Estimates vs ACS1 Counts",
       subtitle = "Inc-to-Pov, Extended Ctrls, FH-based Est, Ages 0-5",
       x = "Observed ACS 1-year Counts") 

sae_results_incpov_ctrls_mix_612[["agg_comp_fh"]]$agg_comp_plot +
  labs(title = "PUMA-level SAE Estimates vs ACS1 Counts",
       subtitle = "Inc-to-Pov, Extended Ctrls, FH-based Est, Ages 6-12",
       x = "Observed ACS 1-year Counts")
```


```{r compare sums of estimates to PUMA-level with observed PUMA values -- CCAP -- FH vs model for ages 0-5}
# Seems reasonably close especially for thicker--i.e. non 200-300-- groups.
# However, it's (unsurprisingly) shading lower for poverty, which I think reflects
# shrinking down to lower levels. Will want to see if that's addressed with 
# better control sets.

if (str_detect(my_output_tag, "Cook2023")) {
  cat("FH vs Model estimates for CCAP estimates for youth aged 0-5 do not show much difference from each other. Both are particulately poor at estimating children in non-work eligible families with higher income, but these are categories that outside of policy interest.")
}

sae_results_ccdf_wk_pov_05[["agg_comp_fh"]]$agg_comp_plot +
  labs(title = "PUMA-level SAE Estimates vs ACS1 Counts",
       subtitle = "CCAP Elig Cats, Extended Ctrls, FH-based Est, Ages 0-5",
       x = "Observed ACS 1-year Counts") +
  theme(strip.text = element_text(size = 8),
        axis.text  = element_text(size = 8))

sae_results_ccdf_wk_pov_05[["agg_comp_model"]]$agg_comp_plot +
  labs(title = "PUMA-level SAE Estimates vs ACS1 Counts",
       subtitle = "CCAP Elig Cats, Extended Ctrls, Model-based Est, Ages 0-5",
       x = "Observed ACS 1-year Counts") +
  theme(strip.text = element_text(size = 8),
        axis.text  = element_text(size = 8))
```


```{r compare sums of estimates to PUMA-level with observed PUMA values -- CCAP -- FH vs model for ages 6-12}
if (str_detect(my_output_tag, "Cook2023")) {
  cat("FH vs Model estimates for CCAP estimates for youth aged 0-5 do not show much difference from each other. Both are particulately poor at estimating children in non-work eligible families with higher income, but these are categories that outside of policy interest.")
}

sae_results_ccdf_wk_pov_612[["agg_comp_fh"]]$agg_comp_plot +
  labs(title = "PUMA-level SAE Estimates vs ACS1 Counts",
       subtitle = "CCAP Elig Cats, Extended Ctrls, FH-based Est, Ages 6-12",
       x = "Observed ACS 1-year Counts") +
  theme(strip.text = element_text(size = 8),
        axis.text  = element_text(size = 8))

sae_results_ccdf_wk_pov_612[["agg_comp_model"]]$agg_comp_plot +
  labs(title = "PUMA-level SAE Estimates vs ACS1 Counts",
       subtitle = "CCAP Elig Cats, Extended Ctrls, Model-based Est, Ages 6-12",
       x = "Observed ACS 1-year Counts") +
  theme(strip.text = element_text(size = 8),
        axis.text  = element_text(size = 8))

```

```{r compare correlations between puma-level observed and predicted values - fay herriot, eval = developer_mode}
cbind(sae_results_incpov_simple_05[["agg_comp_fh"]]$agg_comp_cor     %>% mutate(simple_05     = round(V1, 3)) %>% select(-V1),
      sae_results_incpov_simple_612[["agg_comp_fh"]]$agg_comp_cor    %>% mutate(simple_612    = round(V1, 3)) %>% select(simple_612),
      sae_results_incpov_ctrls_mix_05[["agg_comp_fh"]]$agg_comp_cor  %>% mutate(ctrls_mix_05  = round(V1, 3)) %>% select(ctrls_mix_05),
      sae_results_incpov_ctrls_100_05[["agg_comp_fh"]]$agg_comp_cor  %>% mutate(ctrls_100_05  = round(V1, 3)) %>% select(ctrls_100_05),
      sae_results_incpov_ctrls_mix_612[["agg_comp_fh"]]$agg_comp_cor %>% mutate(ctrls_mix_612 = round(V1, 3)) %>% select(ctrls_mix_612)) %>% 
  kable(caption = "Correlation of PUMA-level Observed and SAE Values from the Fay-Herriot Method")
```

```{r compare correlations between puma-level observed and predicted values - model, eval = developer_mode}
cbind(sae_results_incpov_simple_05[["agg_comp_model"]]$agg_comp_cor     %>% mutate(simple_05     = round(V1, 3)) %>% select(-V1),
      sae_results_incpov_simple_612[["agg_comp_model"]]$agg_comp_cor    %>% mutate(simple_612    = round(V1, 3)) %>% select(simple_612),
      sae_results_incpov_ctrls_mix_05[["agg_comp_model"]]$agg_comp_cor  %>% mutate(ctrls_mix_05  = round(V1, 3)) %>% select(ctrls_mix_05),
      sae_results_incpov_ctrls_100_05[["agg_comp_model"]]$agg_comp_cor  %>% mutate(ctrls_100_05  = round(V1, 3)) %>% select(ctrls_100_05),
      sae_results_incpov_ctrls_mix_612[["agg_comp_model"]]$agg_comp_cor %>% mutate(ctrls_mix_612 = round(V1, 3)) %>% select(ctrls_mix_612)) %>% 
  kable(caption = "Correlation of PUMA-level Observed and SAE Values from the Model-Only Method")
  # Note that these estimates will naturally be below those of the FH method, which blends the 
  # model estimates with the observe SAE values themselves
  # But also, in a sense, this is a cleaner perspective on the results of the model
```

#### Redistribute SAE

Our non-canonical application of the SAE method requires several adjustments. First, the PUMA-level means are imprecise. In part, it is not guaranteed from the SAE method itself, although the diagnostics above suggest that the counts are closer. But also, our use of the ACS 1-year microdata is thin, and there are cases--notably for income bands--where ACS 1-year 5% data are available (i.e. the aggregate tables) and we have better anchors. 

Second, because our model makes use of left-hand side quantities that have reduced variance, as PUMA-level aggregates rather than truly tract-level estimates, our SAE estimates become compressed. See below. 

```{r compare SAE estimate to acs5}
compare_sae_lt100fpl <- function(sae_spec, title, xlab, ylab) {
  
  # To handle the fact that different age runs of the SAE use different age-specific
  # income-to-poverty constructions, we remove the age-related part of the field name
  old_incpov_names <- str_subset(cn(sae_spec), "incpov.+est")
  new_incpov_names <- str_replace(old_incpov_names, "(incpov).+(_r.+_est)", "\\1\\2")
  
  comp_data <-
    sae_spec[vc_value %in% c("0%-50%", "50%-100%", "0%-100%")] %>% 
    setnames(old_incpov_names,
             new_incpov_names) %>% 
    mutate(incpov_r0to100_est = incpov_r0to50_est + incpov_r50to100_est) %>% 
    select(GEOID, share_fh_trunc, incpov_r0to100_est) %>% 
    group_by(GEOID) %>% 
    # Sum across 0-50% and 50-100% FPL, within tract
    summarize(share_fh_trunc = sum(share_fh_trunc),
              incpov_r0to100_est = unique(incpov_r0to100_est)) %>% 
    mutate(label = paste0("GEOID: ", GEOID))
  
  rho <- with(comp_data, cor(share_fh_trunc, incpov_r0to100_est, use = "pairwise")) %>% round(2)
  
  # If tracts within a specific auxiliary geography are of interest, merge that
  # information in and style the plotted points
  if ("my_aux_geo_focal_val" %in% objects()) {
    comp_data <- 
      comp_data %>% 
      merge(geo_crosswalk[j = .(GEOID, aux_geo_label)] %>% unique(),
          by = "GEOID",
          all.x = TRUE)
    plot_base <- 
      ggplot(comp_data,
             aes(x = incpov_r0to100_est,
                 y = share_fh_trunc,
                 #group = label,
                 color = aux_geo_label == my_aux_geo_focal_val)) + 
      scale_color_manual(breaks = c(FALSE, TRUE),
                         values = c("gray", "blue"),
                         labels = c(paste0("Non-", my_aux_geo_focal_val), 
                                    my_aux_geo_focal_val))
  } else {
    plot_base <- 
      ggplot(comp_data,
             aes(x = incpov_r0to100_est,
                 y = share_fh_trunc)) #,#group = label
  }
  
  # Generate the plot
  sae_lt100fpl_comp_plot <- 
    plot_base +
    geom_point(alpha = 0.1) +
    geom_abline(color = "red") +
    geom_smooth(color = "blue") + 
    scale_x_continuous(labels = percent) +
    scale_y_continuous(labels = percent) +
    labs(title = title,
         x = xlab,
         y = ylab) +
    annotate("text", x = 0.1, 
             y = 0.6, 
             label = glue("rho = {rho}")) +
    theme_minimal() +
    theme(legend.position = "none",
          title = element_text(size = 10))
  
  sae_lt100fpl_comp_plot
  
  #ggplotly(sae_lt100fpl_comp_plot, tooltip = "group")
}
```

<!-- Next, we want to understand the source of these idiosyncracies. Direct inspection suggested that our first handful of outlier tracts had very low poverty but also very low rates of labor force participation and employment.  -->

```{r build plot of comparisons}
# /*\ Add appropriate figure titles with interpretation of results, using the
# same condition on `my_output_tag` as below

# /!\ Note -- by and large, the SAE estimates are notably compressed relative to
# the ACS5 measures. The reason for this is that the SAE estimates are anchored
# on PUMA-level values from the ACS1. Although the model component of the SAE
# estimates uses ACS5 measures for x_g, which capture the higher level of variation,
# the fact that the model is predicting s_G (i.e., large geographic measures that
# have some variation averaged away) mean that the estimated betas will put
# the SAE predictions into a narrower space. While using x_G for estimate would
# yield unbiased betas, the use of x_g to predict those betas would likely yield
# unstable predictions, e.g. ones that could range below 0% or above 100%. The
# current modeling choice substitutes a certain amount of compression (and thus 
# bias) for stability. The GMM method described in the "00" script would solve
# the need to make this tradeoff, giving a more natural structure to the relationship
# of PUMA-level measures, and tract-level predictors.
# 
# (Note that, previously, we saw more consistency in this diagnostic. We believe
# that was erroneous, due to the fact that (1) ACS 5-year measures of overall poverty
# rates were compared with SAE estimates that were properly estimated for ages
# 0-5; and that (2) poverty rates for families with young children are higher
# than those for other families. Thus, the compressed SAE estimates were compared
# to a measure for a different population that featured more similarity.

diagnostic_note <- 
  glue("NOTE: This comparison is not expected to show consistency, given\n",
       "our expectation of the SAE method showing some compression")

compare_sae_lt100fpl(
  sae_results_incpov_simple_05$sae_out,
  title = case_when(my_output_tag == "IL"       ~ diagnostic_note,
                    my_output_tag == "Cook2022" ~ diagnostic_note,
                    TRUE ~ ""),
  xlab = "Observed ACS 5-year Poverty Rate, Ages 0-5 -- Simple Spec",
  ylab = "SAE Estimate of Poverty Rate, Age 0-5"
) +
  theme_minimal()

compare_sae_lt100fpl(
  sae_results_incpov_ctrls_mix_05$sae_out,
  title = case_when(my_output_tag == "IL"       ~ diagnostic_note,
                    my_output_tag == "Cook2022" ~ diagnostic_note,
                    TRUE ~ ""),
  xlab = "Observed ACS 5-year Poverty Rate, Ages 0-5 -- Rich Spec - Mixed Interval FPL",
  ylab = "SAE Estimate of Poverty Rate, Ages 0-5"
) +
  theme_minimal()

compare_sae_lt100fpl(
  sae_results_incpov_ctrls_100_05$sae_out,
  title = case_when(my_output_tag == "IL"       ~ diagnostic_note,
                    my_output_tag == "Cook2022" ~ diagnostic_note,
                    TRUE ~ ""),
  xlab = "Observed ACS 5-year Poverty Rate, Ages 0-5 -- Rich Spec - FPL by 100s",
  ylab = "SAE Estimate of Poverty Rate, Ages 0-5"
) +
  theme_minimal()

compare_sae_lt100fpl(
  sae_results_incpov_ctrls_mix_612$sae_out,
  title = case_when(my_output_tag == "IL"       ~ diagnostic_note,
                    my_output_tag == "Cook2022" ~ diagnostic_note,
                    TRUE ~ ""),
  xlab = "Observed ACS 5-year Poverty Rate, Ages 6-12 -- Rich Spec - Mixed Interval FPL",
  ylab = "SAE Estimate of Poverty Rate, Ages 6-12") +
  theme_minimal()
```

```{r investigate select outliers, eval = developer_mode}

sae_results_incpov_ctrls_mix_05$sae_out %>%
  .[GEOID %in% c("17031280900"),
    .(GEOID, vc_value, share_direct, share_fh)] %>% 
  .[order(GEOID, vc_value)]
```

Thus, we undertake a further adjustment, by:

1. aligning our SAE estimates for each income-to-poverty category to the ACS 1-year 5% sample values for each PUMA, and
2. expanding the variance of the SAE estimates to equal the variance of ACS 5-year analogs.

In this way, we anchor the distribution of our new estimates in the most reliable and most recent values that are available.

```{r load acs5 measures relevant for aligning to sae categories}
load(file = glue("{output_path}acs5_ccdf_{my_output_tag}.Rda"))
```

```{r function to select relevant income-to-poverty measures}
incpovs <- 
  acs5_incpov_byage$inc_to_pov_ratio %>% 
  unique() %>% 
  sort_by_char_nums() %>% 
  sort()
incpov_bounds <- str_extract_all(incpovs, "\\d+")
dfIncPovCutoffs <- 
  data.frame(incpov = incpovs,
             left  = sapply(incpov_bounds, function(x) x[1]) %>% as.numeric(),
             right = sapply(incpov_bounds, function(x) x[2]) %>% as.numeric())

get_closest <- function(val, ref) {
  ref <- setdiff(ref, NA)
  absdev <- abs(val - ref)
  ref[which(absdev == min(absdev))]
}

return_incpov_fields_bounds <- function(low, high) {
  closest_low  <- get_closest(low,  dfIncPovCutoffs$left)
  closest_high <- get_closest(high, dfIncPovCutoffs$right)
  
  out <- filter(dfIncPovCutoffs, left >= closest_low)
  if (!is.na(high) & !is.infinite(high)) {
    out <- filter(out, right <= closest_high)
  }
  return(out$incpov)
}

if (FALSE) {
  return_incpov_fields_bounds(0, 100)
  return_incpov_fields_bounds(1, 99)
  return_incpov_fields_bounds(0, 101)
  return_incpov_fields_bounds(500, NA)
}
```

```{r notes for calculating ccdf eligibility below}
# Because we do not know the joint distribution of these income- and work-eligibility
# phenomena, we use microdata for some calibration. We use the phi calculation of
# correlation for bernoulli random variables, which is just the pearson correlation.
# We first confirm that.
if (FALSE) {
  library(psych)
  df <- data.frame(x = c(1, 1, 0, 1),
                   y = c(1, 0, 0, 1))
  
  with(df, cor(x, y))
  
  mat <- matrix(c(2, 1, 0, 1), ncol = 2, byrow = TRUE)
  phi(mat, digits = 7)
  # This yields the same result
}
```


```{r results = 'asis'}
make_acs5_equiv <- function(sae_ests, sae_age, verbose = FALSE) {
  
  ### Read SAE elements
  # vc values to match
  vc_vals <- unique(sae_ests$vc_value)
  
  # age for subsetting
  acs5_ages <- 
    case_when(sae_age == "05"  ~ "Age0to6",
              sae_age == "612" ~ "Age6to11")
  acs1_ages <- 
    case_when(sae_age == "05"  ~ c(0, 5),
              sae_age == "612" ~ c(6, 12))
  
  acs5_ests <- NULL
  for (vc in vc_vals) {
    
    if (verbose) print(glue("\n\nvc -- {vc}\n"))
    # Parse income values
    # <move left/right function to the ACS5 function methods>
    incpov_bounds <- str_extract_all(vc, "\\d+") %>% unlist() %>% as.numeric()
    incpov_left  <- incpov_bounds[1]
    incpov_right <- incpov_bounds[2] %>% replace_na(Inf)
    
    # Aggregate incpov as relevant
    incpov_cat_keep <- return_incpov_fields_bounds(incpov_left, incpov_right)
    acs5_incpov_vc <- 
      acs5_incpov_byage %>% 
      filter(inc_to_pov_ratio %in% incpov_cat_keep,
             age == acs5_ages) 
    
    if (verbose) print(glue("  ACS5 income cats are: {paste(levels(sort_by_char_nums(acs5_incpov_vc$inc_to_pov_ratio)), collapse = ', ')}, and ages are {paste(unique(acs5_incpov_vc$age), collapse = ',')}.\n"))
    acs5_incpov_vc <- 
      acs5_incpov_vc %>% 
      summarize(rInc = sum(r),
                wAcs5 = sum(w),
                .by = GEOID)
    
    ### Calculate joint income-and-work status work eligibility is relevant ###
    # The measures that we have calculated from pulls of the ACS 5-year data are
    # only marginal distributional values. However, we want estimates of the joint
    # value, i.e. kids in families based on both income and work eligibility status.
    # We can use calculations from the phi coefficient (https://en.wikipedia.org/wiki/Phi_coefficient)
    # in the microdata above with these marginal values to estimate the likelihood of 
    # the joint work eligible x income eligible condition.
    
    # Using notation from that wikipedia article, note that the n11, n01, n10, n00
    # values could be considered rates instead of counts, where n11 is the value
    # of interest, and n1., n0., n.1, and n.0 are the marginal values that we
    # obtain from the ACS 5-year data. Call these n1. = p, n0. = (1-p), n.1 = q,
    # n.0 = (1-q), and note that n = 1, as the sum of marginal probabilities
    # must equal 100% probability. Then, the second equation for phi under the 
    # "Definition" section which reads as follow, can be reduced:
    #   phi = (n11 - n1.*n.1) / sqrt(n1.*n.1*(n - n1.)*(n - n.1))
    #       = (1*n11 - p*q) / sqrt(p*q*(1-p)*(1-q))
    # And thus:
    #   n11 = phi * sqrt(p*q*(1-p)*(1-q)) + pq
    # Here, n11 is the quantity of interest--the joint probability of income and
    # work eligibility status--and where we have phi calculated. Note that phi=0 
    # corresponds to the case of zero correlation, and would result in n11 = pq
    # which--as the joint probability case where both random variables are 
    # independent--confirms common expectations.
    
    if (str_detect(vc, "Elig")) {
      # Subset the relevant rows from the ACS5 data
      acs5_elig <- 
        case_when(str_detect(vc, "NotWorkElig")     ~ "NotWorkEligFam",
                  str_detect(vc, "(^|\\W)WorkElig") ~ "WorkEligFam")
      
      acs5_workelig <- 
        acs5_workelig_withKids %>% 
        filter(status == acs5_elig)
      
      if (verbose) cat(glue("  ACS5 work elig status is: {paste(unique(acs5_workelig$status), collapse = ', ')}.\n"))
      
      acs5_workelig <- 
        acs5_workelig %>%
        select(GEOID, rWork = r)
      
      # Calculate phi by PUMA for relevant incpov and work elig
      ccdf_elig_field <- glue("ccdf_elig_incratio_{local_ccdf_incratio_base}")
      acs1_forPhi <-
        copy(acs1_child) %>% 
        .[acs1_ages[1] <= AGEP & AGEP < acs1_ages[2],
          j = .(AGEP, PUMA, fam_incpov_ratio, all_work_sch,
                inc_measure  = 1*(between(fam_incpov_ratio*100, incpov_left, incpov_right)),
                work_measure = case_match(rep(acs5_elig, .N),
                                          "NotWorkEligFam" ~ 1 - all_work_sch,
                                          "WorkEligFam"    ~     all_work_sch))]
      if (verbose) {
        print("Stats for the acs1 data for calculating phi--\n")
        range_paste <- function(x) paste0(range(x, na.rm = T), collapse = "-")
        ranges_forPhi <- 
          acs1_forPhi[j = .(vc = vc,
                            incpov_range = range_paste(fam_incpov_ratio),
                            is_work_elig = range_paste(all_work_sch),
                            age_range    = range_paste(AGEP)),
                      by = .(inc_measure, work_measure)] %>% 
          arrange(-inc_measure, -work_measure)
        print(ranges_forPhi)
      }
      
      # Calculate `phi` by PUMA
      phi_p <- 
        acs1_forPhi %>% 
        .[j = .(phi_p = cor(inc_measure, work_measure)),
          by = PUMA] %>% 
        .[j = phi_p := replace_na(phi_p, meanNA(phi_p))]
      
      # For comparison, calculate `phi` across all PUMAs
      phi <- 
        acs1_forPhi %>% 
        .[j = .(phi = cor(inc_measure, work_measure))] %>% 
        .[j = phi := replace_na(phi, meanNA(phi))] %>% 
        pull(phi)
      
      # Estimate combo
      acs5_est_vc <- 
        acs5_incpov_vc %>% 
        merge(acs5_workelig, 
              by = "GEOID") %>% 
        merge(geo_crosswalk %>% select(GEOID, PUMA) %>% unique(),
              by = "GEOID") %>% 
        merge(phi_p,
              by = "PUMA") %>% 
        as.data.table() %>% 
        .[j = .(GEOID,
                vc_value = vc,
                wAcs5,
                share_acs5 = 
                  phi_p * sqrt(rInc*rWork*(1-rInc)*(1-rWork)) +
                  rInc*rWork,
                share_acs5_phi = 
                  phi   * sqrt(rInc*rWork*(1-rInc)*(1-rWork)) +
                  rInc*rWork,
                share_acs5_indep = rInc*rWork)]
    } else {
      acs5_est_vc <- 
        acs5_incpov_vc %>% 
        as.data.table() %>% 
        .[j = .(GEOID,
                vc_value = vc,
                share_acs5 = rInc)]
    }
    acs5_ests <- bind_rows(acs5_ests, acs5_est_vc)
  }
  sae_ests_aug <- 
    sae_ests %>% 
    merge(acs5_ests,
          by = c("GEOID", "vc_value"),
          all.x = TRUE)
  
  return(sae_ests_aug)
}
```

```{r run test of calculation of decompression statistics, results = 'asis'}
if (developer_mode) {
  # Some suppressed code to make interactive troubleshooting easier
  if (FALSE) {
      debug(make_acs5_equiv)
    undebug(make_acs5_equiv)
  }
  sae_results_ccdf_test <- make_acs5_equiv(sae_results_ccdf_wk_pov_05$sae_out, "05", verbose = TRUE)
} 
```

```{r how much difference does use of phi vs phi_p make}
if (developer_mode) {
  
  # Compare the results from using phi_p and phi
  # Hypothesis is that for smaller cells, the phi-derived estimates will be more stable
  sae_results_ccdf_test %>% 
    ggplot(aes(x = share_acs5, 
               y = share_acs5_phi)) +
    geom_abline() + 
    geom_point(alpha = 0.1) + 
    geom_smooth() +
    facet_wrap(~vc_value) +
    labs(title    = "Comparison of ACS5 Estimates Derivations",
         subtitle = "Use of PUMA-specific calcs of Phi vs Full-state")
    # Result: seems that the results are fairly stable
  
}
```


```{r compare acs5 estimates with sae estimates}
if (developer_mode) {
  
  # Point is: the values should hopefully be basically consistent, but the 
  # variance of the acs5 should be substantially higher, which is what we rely
  # on for decompressing the sae estimates
  sae_results_ccdf_test %>% 
    group_by(vc_value) %>% 
    mutate(label = glue("{vc_value}\nrho = {round(cor(share_model_trunc, share_acs5, use = 'pairwise'), 2)}\nratio of ",
                        "var = {round(varNA(share_acs5)/varNA(share_model_trunc), 2)}")) %>% 
  ggplot(aes(x = share_model_trunc,
             y = share_acs5)) +
    geom_abline() + 
    geom_point(alpha = 0.1) +
    geom_smooth() + 
    facet_wrap(~ label) +
    labs(title    = "Comparison of Tract-Level ACS5 Estimates vs SAE",
         subtitle = glue("Expectations: disagreements due to lag of ACS5 and other assumptions\n",
                         "but higher variance of ACS5, which is the key to decompression"))
}
```


```{r Compare ACS1 and ACS5 at the PUMA level}

if (developer_mode) {
  
  # As a reality check, see if the ACS5 estimates are basically consistent with
  # calculations from the ACS1 microdata. Both the lag of the former and general
  # sampling variation would cause some disagreement even if the assumptions
  # were basically correct. Still, we hope for some strong correspondence in
  # aggregate.
  ccdf_inc_thresh_fpl <- get_fpl_thresh_for_ccdf()
  acs1_ccdf_p <- 
    acs1_child %>% 
    .[between(AGEP, 0, 6),
      j = .(acs1_ccdf = 
              sum(PWGTP[fam_incpov_ratio < ccdf_inc_thresh_fpl/100 & 
                          all_work_sch == TRUE]) / sum(PWGTP),
            acs1_ccdf_inc  = sum(PWGTP[fam_incpov_ratio < ccdf_inc_thresh_fpl/100]) / sum(PWGTP),
            acs1_ccdf_work = sum(PWGTP[all_work_sch == TRUE])                       / sum(PWGTP)),
      by = PUMA] %>% 
    .[j = acs1_ccdf_indep := acs1_ccdf_inc*acs1_ccdf_work]

  
  # Examine estimates of CCDF eligibility
  sae_results_ccdf_test %>% 
    filter(vc_value %in% c("WorkElig_0%-100%", glue("WorkElig_100%-{ccdf_inc_thresh_fpl}%"))) %>% 
    summarize(acs5_ccdf = sum(share_acs5),
              acs5_ccdf_w = sum(wAcs5),
              .by = GEOID) %>%  
    merge(geo_crosswalk %>% select(GEOID, PUMA) %>% unique(),
          by = "GEOID") %>% 
    summarize(acs5_ccdf = weighted.mean(acs5_ccdf, acs5_ccdf_w),
              .by = PUMA) %>% 
    merge(acs1_ccdf_p,
          by = "PUMA") %>% 
    ggplot(aes(x = acs1_ccdf,
               y = acs5_ccdf)) +
    geom_abline() +
    geom_hline(yintercept = 0) +
    geom_vline(xintercept = 0) +
    geom_point() +
    geom_smooth() +
    labs(title    = "Comparison of PUMA-level CCDF Estimates from ACS5 vs ACS1",
         subtitle = "Expectations: although they likely won't line up, there's ideally decent correspondence")
  
  # Examine income eligibility at the PUMA level
  acs5_incpov_byage %>% 
    filter(age == "Age0to6", 
           inc_to_pov_ratio %like% "0to50|50to|75to|100to|125to|150to|175to|185to") %>% 
    summarize(acs5_ccdf_inc   = sum(r),
              acs5_ccdf_inc_w = mean(w),
              .by = GEOID) %>%  
    merge(geo_crosswalk %>% select(GEOID, PUMA) %>% unique(),
          by = "GEOID") %>% 
    summarize(acs5_ccdf_inc = weighted.mean(acs5_ccdf_inc, acs5_ccdf_inc_w),
              .by = PUMA) %>% 
    merge(acs1_ccdf_p,
          by = "PUMA") %>% 
    ggplot(aes(x = acs1_ccdf_inc,
               y = acs5_ccdf_inc)) +
    geom_abline() +
    geom_hline(yintercept = 0) +
    geom_vline(xintercept = 0) +
    geom_point() +
    geom_smooth() +
    labs(title    = "Comparison of PUMA-level CCDF Income Estimates from ACS5 vs ACS1",
         subtitle = "Expectations: although they likely won't line up, there's ideally decent correspondence")
  
  acs5_workelig_withKids %>% 
    filter(status == "WorkEligFam") %>% 
    merge(geo_crosswalk %>% select(GEOID, PUMA) %>% unique(),
          by = "GEOID") %>% 
    summarize(acs5_ccdf_work = weighted.mean(r, w),
              .by = PUMA) %>% 
    merge(acs1_ccdf_p,
          by = "PUMA") %>% 
    ggplot(aes(x = acs1_ccdf_work,
               y = acs5_ccdf_work)) +
    geom_abline() +
    geom_hline(yintercept = 0) +
    geom_vline(xintercept = 0) +
    geom_point() +
    geom_smooth() +
    labs(title    = "Comparison of PUMA-level CCDF Work Eligibility Estimates from ACS5 vs ACS1",
         subtitle = "Expectations: although they likely won't line up, there's ideally decent correspondence")
}
```


```{r calculate variance of acs5 analogs to each income-to-poverty measure}
pop_long <- 
  pop_by_age %>% 
  #filter(year == base_year) %>% 
  rename(GEOID = GEOID_TR20) %>% 
  pivot_longer(cols = -c(GEOID, year)) %>% 
  mutate(age = str_replace(name, ".+_(\\d+to\\d+).+", "\\1")) %>% 
  select(GEOID, year, age, pop = value)

```

```{r misc functions for use in redistribution}
bound <- function(x, lower = 0, upper = 1) {
  x[x < lower] <- lower
  x[x > upper] <- upper
  return(x)
}

if (FALSE) {
  bound(c(-1, 0.2, 0.8, 5))
}
```


```{r function for performing redistribution}
# Values for easy interactive debugging
if (FALSE) {
  debug(redis_sae)
  undebug(redis_sae)
  sae_redis_incpov <- redis_sae(sae_results_incpov_ctrls_100_05, "05"); View(sae_redis_incpov$sae_out, "redis_incpov")
  sae_redis_ccdf   <- redis_sae(sae_results_ccdf_wk_pov_05, "05");      View(sae_redis_ccdf$sae_out,   "redis_ccdf")
}

redis_sae <- function(sae_to_redis, sae_age) {
  
  sae_to_redis_aug <- make_acs5_equiv(sae_to_redis$sae_out, sae_age)
  
  pop_age <- case_when(sae_age == "05"  ~ "0to5", 
                       sae_age == "612" ~ "6to12")
  acs_age <- case_when(sae_age == "05"  ~ "lt6", 
                       sae_age == "612" ~ "6to11")
  
  redis_out <- 
    sae_to_redis_aug %>% 
    merge(pop_long %>% filter(age == pop_age, year == base_year),
          by = "GEOID",
          all.x = TRUE) %>% 
    group_by(PUMA, vc_value) %>% 
    mutate(
      # This mutate() step performs redistribution of the SAE estimates via an
      # "affine" transformation (i.e. adding/subtract/multiplication), as:
      #   new = (sae - mean(sae))*(acs5_sd/sae_sd) + mean(old)

      # We attempt this both in "levels", i.e. the existing percentage-based units
      # and with values transformed to the logistic scale. The theory for using the
      # logistic-transformed values is that they exist on the full real number
      # line, and thus would avoid issues of values being redistributed below 0%
      # and above 100%.
      
      ### Set up affine transformation
      # Center values to have mean 0
      share_model_trunc_m0 = share_model_trunc - mean(share_model_trunc),
      share_fh_trunc_m0    = share_fh_trunc    - mean(share_fh_trunc),
      share_acs5_m0        = share_acs5        - mean(share_acs5),
      
      # Calculate std-deviations of shares
      share_model_trunc_sd = sdNA(share_model_trunc),
      share_fh_trunc_sd    = sdNA(share_fh_trunc),
      share_acs5_sd        = sdNA(share_acs5),
      
      ## Perform logistic transformation, and get sd's
      # For some reason, it's producing NAs to combine the separate calculations
      # below. Separating them, as we have, avoids this.
      share_model_trunc_log      = qlogis(share_model_trunc),
      share_model_trunc_log_mean = meanNA(share_model_trunc_log),
      share_model_trunc_log_m0   = share_model_trunc_log - share_model_trunc_log_mean,
      share_model_trunc_log_sd   = sdNA(share_model_trunc_log_m0),
      
      share_fh_trunc_log      = qlogis(share_fh_trunc),
      share_fh_trunc_log_mean = meanNA(share_fh_trunc_log),
      share_fh_trunc_log_m0   = share_fh_trunc_log - share_fh_trunc_log_mean,
      share_fh_trunc_log_sd   = sdNA(share_fh_trunc_log_m0),
      
      share_acs5_log      = qlogis(share_acs5),
      share_acs5_log_mean = meanNA(share_acs5_log),
      share_acs5_log_m0   = share_acs5_log - share_acs5_log_mean,
      share_acs5_log_sd   = sdNA(share_acs5_log_m0),
      
      # Build variance inflation factors for decompressing the estimates
      infl_model_pct = share_acs5_sd     / share_model_trunc_sd,
      infl_model_log = share_acs5_log_sd / share_model_trunc_log_sd,
      infl_fh_pct    = share_acs5_sd     / share_fh_trunc_sd,
      infl_fh_log    = share_acs5_log_sd / share_fh_trunc_log_sd,
      
      # Redistribute using variance and mean adjustment
      share_model_redis_pct = 
        (infl_model_pct * share_model_trunc_m0) + mean(share_model_trunc),
      
      share_fh_redis_pct    =
        (infl_fh_pct    * share_fh_trunc_m0)    + mean(share_fh_trunc),
      
      share_model_redis_log = 
        
        plogis(infl_model_log * share_model_trunc_m0  + share_model_trunc_log_mean),
      
      share_fh_redis_log    = 
        plogis(infl_fh_log    * share_fh_trunc_m0     + share_fh_trunc_log_mean),
      
      # Bound results within the interval [0,1]
      share_model_redis_pct01 = bound(share_model_redis_pct),
      share_fh_redis_pct01    = bound(share_fh_redis_pct),
      share_model_redis_log01 = bound(share_model_redis_log),
      share_fh_redis_log01    = bound(share_fh_redis_log)
    ) %>% 
    group_by(GEOID) %>% 
    mutate(
      share_model_redis_pct01_infl = share_model_redis_pct01 / sum(share_model_redis_pct01),
      share_fh_redis_pct01_infl    = share_fh_redis_pct01    / sum(share_fh_redis_pct01),
      share_model_redis_log01_infl = share_model_redis_log01 / sum(share_model_redis_log01),
      share_fh_redis_log01_infl    = share_fh_redis_log01    / sum(share_fh_redis_log01),
    ) %>% 
    as.data.table()
  
  # Output the results to the `sae_out` slot of the input SAE
  sae_to_redis[["sae_out_decompress"]] <- redis_out
  
  return(sae_to_redis)
}
```


```{r implement redistributions}
for (spec in setdiff(specs, "ccdf_wk_sp_05")) {
  #print(spec)
  sae_name <- glue("sae_results_{spec}")
  # The age is given by the final numbers in the name of the spec
  sae_age <- str_extract(spec, "\\d+$")
  
  assign(glue("sae_results_{spec}"),
         redis_sae(get(sae_name), sae_age))
}

# Check on success
if (FALSE) {
  str_subset(cn(sae_results_incpov_ctrls_100_05$sae_out_decompress), "_infl$")
  sae_results_incpov_ctrls_100_05$sae_out_decompress %>% 
    select(GEOID, vc_value, 
           share_model_trunc, share_model_redis_pct, share_model_redis_pct01_infl)
  
  str_subset(cn(sae_results_ccdf_wk_pov_05$sae_out_decompress), "_infl$")
  sae_results_ccdf_wk_pov_05$sae_out_decompress %>% 
    select(GEOID, vc_value, 
           share_model_trunc, share_model_redis_pct, share_model_redis_pct01_infl)
}

```

```{r save SAE ingredients}

if (rerun_sae) {
  # Organize contents
  sae_aux_data_acs5 <- acs5tract_sae
  sae_controls_list <- list(sae_controls_simple_05,
                            sae_controls_simple_612,
                            sae_controls_added_2534_05,
                            sae_controls_added_2534_612)
  results <- paste0("sae_results_", specs)
  
  # Save the bundle of objects
  save(list = c(results, "specs", "incpov_ref_value", "sae_controls_list", "sae_aux_data_acs5"),
       file = glue("{output_path}sae_sensitivity_estimates_{my_output_tag}.Rda"))
  
} else {
  load(file = glue("{output_path}sae_sensitivity_estimates_{my_output_tag}.Rda")) 
}
```

```{r extract estimate tables from sae output}
for (spec in specs) {
  stem <- glue("sae_results_{spec}")
  assign(glue("{stem}_out"),
         get(stem)[["sae_out_decompress"]])
}
```

```{r pick a single SAE sensitivity to investigate}
check_redis_sae <- sae_results_incpov_ctrls_100_05$sae_out_decompress
check_stats_puma <- 
  acs1_child %>% 
  .[between(AGEP, 0, 5),
    j = .(wgt = sum(PWGTP)),
    by = .(PUMA, vc_value = fam_incpov_ratio_cat_by100)] %>% 
  .[j = share_acs1 := wgt / sum(wgt),
          by = PUMA] %>% 
  select(-wgt)
```

```{r examine overages}
# Examine how the sums look
overs <- 
  check_redis_sae %>% 
  group_by(GEOID, PUMA) %>% 
  summarize(sums = sum(share_model_redis_pct01),
            check = sum(share_model_redis_pct01_infl))

ggplot(overs,
       aes(x = sums)) +
  geom_density()

# Concentrated in smaller sets of PUMAs?
overs %>% 
  group_by(PUMA) %>% 
  summarize(pct_over = mean(sums>1.5))

```

```{r calculate percentages of estimates falling outside of [0,1], eval = developer_mode}
check_redis_sae %>% 
  group_by(vc_value) %>% 
  summarize(share_model_redis_pct_lt0 = meanNA(share_model_redis_pct < 0) %>% percent(1),
            share_model_redis_pct_gt1 = meanNA(share_model_redis_pct > 1) %>% percent(1),
            
            share_model_redis_log_lt0 = meanNA(share_model_redis_log < 0) %>% percent(1),
            share_model_redis_log_gt1 = meanNA(share_model_redis_log > 1) %>% percent(1),
            
            share_fh_redis_pct_lt0    = meanNA(share_fh_redis_pct    < 0) %>% percent(1),
            share_fh_redis_pct_gt1    = meanNA(share_fh_redis_pct    > 1) %>% percent(1),
            
            share_fh_redis_log_lt0    = meanNA(share_fh_redis_log    < 0) %>% percent(1),
            share_fh_redis_log_gt1    = meanNA(share_fh_redis_log    > 1) %>% percent(1)) %>% 
  pivot_longer(cols = -vc_value) %>% 
  pivot_wider(names_from = vc_value,
              values_from = value)
```



```{r check the results -- final versus acs5, eval = developer_mode}
# This is done to see if we obtain something relatively close to the 45
check_redis_sae %>% 
  ggplot(aes(x = share_acs5,
             y = share_model_redis_pct)) +
  geom_abline() + 
  geom_point(alpha = 0.1) + 
  geom_smooth() +
  geom_hline(yintercept = 0, color = "red") +
  geom_hline(yintercept = 1, color = "red") +
  facet_wrap(~vc_value)

check_redis_sae %>% 
  ggplot(aes(x = share_acs5,
             y = share_model_redis_log)) +
  geom_abline() + 
  geom_point(alpha = 0.1) +
  geom_smooth() +
  geom_hline(yintercept = 0, color = "red") +
  geom_hline(yintercept = 1, color = "red") +
  facet_wrap(~vc_value)

# Check the correlations between each adjustment type and the acs5 -- seems that
# the pct adjustment -- despite violating [0,1] bounds -- respects

check_redis_sae %>% 
  group_by(vc_value) %>% 
  summarize(cor_acs5_pct = cor(share_acs5, share_model_redis_pct, use = "pairwise"),
            cor_acs5_log = cor(share_acs5, share_model_redis_log, use = "pairwise"))
  # True -- they are often clearly better. This is never mind the fact that the 
  # slopes of the former are also closer to 1.
```

```{r check the results -- final_bounded versus acs5, eval = developer_mode}
# This is done to see if we obtain something relatively close to the 45
check_redis_sae %>% 
  ggplot(aes(x = share_acs5,
             y = share_model_redis_pct01_infl)) +
  geom_abline() + 
  geom_point(alpha = 0.1) + 
  geom_smooth() +
  geom_hline(yintercept = 0, color = "red") +
  geom_hline(yintercept = 1, color = "red") +
  facet_wrap(~vc_value)

check_redis_sae %>% 
  ggplot(aes(x = share_model_redis_pct,
             y = share_model_redis_pct01_infl)) +
  geom_abline() + 
  geom_point(alpha = 0.1) +
  geom_smooth() +
  geom_hline(yintercept = 0, color = "red") +
  geom_hline(yintercept = 1, color = "red") +
  facet_wrap(~vc_value)

# Check the correlations between each adjustment type and the acs5 -- seems that
# the pct adjustment -- despite violating [0,1] bounds -- respects

check_redis_sae %>% 
  group_by(vc_value) %>% 
  summarize(cor_acs5_pct = cor(share_acs5,            share_model_redis_pct01_infl, use = "pairwise"),
            cor_acs5_log = cor(share_model_redis_pct, share_model_redis_pct01_infl, use = "pairwise"))
  # True -- they are often clearly better. This is never mind the fact that the 
  # slopes of the former are also closer to 1.
```

```{r how does the adjusted values compare with the original, eval = developer_mode}
check_redis_sae %>% 
  ggplot(aes(x = share_model_trunc,
             y = share_model_redis_pct01_infl)) +
  geom_abline() + 
  geom_point(alpha = 0.1) +
  geom_smooth() +
  facet_wrap(~vc_value)
  # In short -- on average, they're close to the 45

```

```{r examine distributions and correlations of original and final SAE vs acs5}
comp_plot <- function(field1, field2) {
  check_redis_sae %>% 
  filter(!is.na(get(field1)),
         !is.na(get(field2))) %>% 
  ggplot(aes(x = get(field1),
             y = get(field2))) +
  geom_abline() + 
  geom_point(alpha = 0.1) +
  geom_smooth() + 
  scale_y_continuous(limits = c(0, 1)) +
  labs(x = field1, y = field2) +
  facet_wrap(~vc_value)
}

comp_plot("share_acs5", "share_model_trunc")
comp_plot("share_acs5", "share_model_redis_pct")
comp_plot("share_acs5", "share_model_redis_pct01_infl")

check_redis_sae %>% 
  group_by(vc_value) %>% 
  summarize(cor_acs5_sae  = cor(share_acs5, share_model_trunc,            use = "pairwise") %>% round(3),
            cor_acs5_pct = cor(share_acs5, share_model_redis_pct, use = "pairwise") %>% round(3),
            cor_acs5_infl = cor(share_acs5, share_model_redis_pct01_infl, use = "pairwise") %>% round(3),
            
            var_acs5 = varNA(share_acs5) %>% round(3),
            var_sae  = varNA(share_model_trunc) %>% round(3),
            var_pct = varNA(share_model_redis_pct) %>% round(3),
            var_infl = varNA(share_model_redis_pct01_infl) %>% round(3)) %>% 
  ungroup()
  # Shows that correlations have dropped somewhat with each manipulation, but 
  # the variances are substantially more realistic, which is critical
  # Note -- these comparisons don't capture the greater alignment with ACS 1
```

```{r check final accordance with PUMA-level statistics, eval = developer_mode}
# Here, we calculate population-weighted averages, for comparison with PUMA-level
# averages directly from the ACS 1
est_vs_acs1 <- 
  check_redis_sae %>% 
  ungroup() %>%
  select(GEOID, PUMA, vc_value, pop, 
         share_acs5, share_model_trunc, share_model_redis_pct,
         share_model_redis_pct01_infl) %>% 
  pivot_longer(cols = -c(GEOID, PUMA, vc_value, pop),
               names_to = "method",
               values_to = "estimate") %>% 
  group_by(PUMA, vc_value, method) %>% 
  summarize(
    # Checking: this should be unique
    estimate_puma = weighted.mean(x = estimate, w = pop)
    ) %>% 
  mutate(
    method = factor(method,
                    levels = c("share_acs5", 
                               "share_model_trunc", 
                               "share_model_redis_pct",
                               "share_model_redis_pct01_infl"))
  ) %>% 
  merge(check_stats_puma,
        by = c("PUMA", "vc_value"))

ggplot(est_vs_acs1,
       aes(x = share_acs1,
           y = estimate_puma,
           color = method)) +
  geom_abline() + 
  geom_point() + 
  geom_smooth() +
  facet_grid(vc_value ~ method) +
  theme(legend.position = "bottom")

  # ACS5 (red) is inconsistent -- which makes sense because they are lagged!
  # The original SAE estimates (yellow) show clear compression
  # The raw redistributed estimates (green) are highly consistent for each band
  # The truncated/reinflated estimates (blue) are also highly consistent for
  #   each band, but of course also respect 0% floor and 100% ceiling for each 
  #   tract. Seem that this advantage did not create problems for PUMA-level 
  #   aggregates.
  # The estimates taken directly from the ACS5 (purple) are also impressively 
  #   consistent with the ACS1

```


#### Compare Tract-Level SAE Estimates to the ACS 5-year


```{r compare measure variance in acs5 vs sae by PUMA, eval = developer_mode}
var_prop_acs5_sae <- 
  sae_results_incpov_ctrls_100_05$sae_out %>% 
  filter(vc_value == "0%-100%") %>% 
  mutate(acs5_pov = incpov_lt6_r0to50_est + incpov_lt6_r50to100_est) %>% 
  group_by(PUMA) %>% 
  summarize(var_acs5_pov = varNA(acs5_pov),
            var_sae_pov  = varNA(share_model_trunc),
            avg_acs5_pov = meanNA(acs5_pov),
            avg_sae_pov  = meanNA(share_model_trunc)) %>% 
  mutate(ratio_var_acs5_sae = var_acs5_pov / var_sae_pov)

ggplot(var_prop_acs5_sae,
       aes(x = reorder(PUMA, ratio_var_acs5_sae),
           y = ratio_var_acs5_sae)) + 
  geom_bar(stat = "identity") +
  geom_hline(yintercept = 1, 
             color = "red") +
  labs(x = "PUMA",
       y = "Ratio of ACS5 Variance to SAE") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5))

ggplot(var_prop_acs5_sae,
       aes(x = avg_acs5_pov,
           y = ratio_var_acs5_sae,
           label = PUMA)) + 
  geom_point() + 
  geom_text(vjust = 1) +
  labs(x = "ACS5 Poverty Rate",
       y = "Ratio of ACS5 Variance to SAE") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5))
  
ggplot(var_prop_acs5_sae,
       aes(x = avg_acs5_pov,
           y = avg_sae_pov,
           label = PUMA)) + 
  geom_point() + 
  geom_abline() +
  geom_text(vjust = 1) +
  labs(title = "Comparison of PUMA-Level ACS5 and SAE Estimates (Simple averages)",
         x = "ACS5 Poverty Rate",
       y = "SAE Poverty Rate") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5))
  # NOTE -- this isn't what we're worried about. We're worried about the ACS1 comparison.
  # See e.g. acs1_child[between(AGEP, 0, 5), j = sum(PWGTP[fam_incpov_ratio_cat_by100 == "0%-100%"])/sum(PWGTP), by = PUMA]
```


<!-- #### Adjust SAE Estimates to Match PUMA-level Aggregates -->

<!-- Note: Although we have piloted the code to ex-post match SAE estimates to 
align with the PUMA-level distribution of children across SAE target categories,
the diagnostic check in the "examine PUMA-level counts" chunks above have 
led us to believe that the sample data are too variable at that level to use as
reliable anchors. -->

```{r eval = FALSE}
# This is not yet implemented, because many of the PUMA-level counts, especially
# for the ccdf specifications, are too thin to match
for (spec in specs) {
  
  my_spec_name <- glue("sae_results_{spec}")
  my_spec <- get(my_spec_name)

  my_spec_infl_puma_fh <- 
    my_spec$agg_comp_fh$agg_comp_data %>% 
    mutate(infl_factor_fh = obs_count_puma / sae_count_puma) %>% 
    select(PUMA, vc_value = vulnerability_cat, infl_factor_fh)
  
  my_spec_infl_puma_model <- 
    my_spec$agg_comp_model$agg_comp_data %>% 
    mutate(infl_factor_model = obs_count_puma / sae_count_puma) %>% 
    select(PUMA, vc_value = vulnerability_cat, infl_factor_model)
  
  
  my_spec$sae_out <- 
    my_spec$sae_out %>% 
    merge(my_spec_infl_puma_fh,    by = c("PUMA", "vc_value")) %>% 
    merge(my_spec_infl_puma_model, by = c("PUMA", "vc_value")) %>% 
    # Use the inflation factor on the counts for both FH and model estimates
    mutate(vc_fh_count_trunc_infl       = infl_factor_fh*vc_fh_count_trunc,
           vc_fh_count_se_trunc_infl    = infl_factor_fh*vc_fh_count_se_trunc,
           vc_model_count_trunc_infl    = infl_factor_model*vc_model_count_trunc,
           vc_model_count_se_trunc_infl = infl_factor_model*vc_model_count_se_trunc) %>% 
    # Recalculate the corresponding shares
    group_by(GEOID) %>% 
    mutate(share_fh_trunc_infl    = vc_fh_count_trunc_infl    / sum(vc_fh_count_trunc_infl),
           share_model_trunc_infl = vc_model_count_trunc_infl / sum(vc_model_count_trunc_infl),
           # Calculate ratios between the prior share calculation and the new one. 
           # We use this to also inflate the accompanying SEs.
           # /!\ This seems like the right approach, but deserves more formality
           share_fh_trunc_infl_ratio    = share_fh_trunc_infl    / share_fh_trunc,
           share_model_trunc_infl_ratio = share_model_trunc_infl / share_model_trunc,
           share_fh_se_trunc_infl    = share_fh_se_trunc    * share_fh_trunc_infl_ratio,
           share_model_se_trunc_infl = share_model_se_trunc * share_model_trunc_infl_ratio)  
  assign(my_spec_name,
         my_spec)
}

save(list = c(results, "specs", "incpov_ref_value", "sae_controls_list", "sae_aux_data_acs5"),
       file = glue("{output_path}sae_sensitivity_estimates_final_{my_output_tag}.Rda"))

# Check inflation worked
if (FALSE) {
  my_spec$sae_out %>% 
    group_by(PUMA, vc_value) %>% 
    summarize(fh_sum    = sum(vc_fh_count_trunc_infl),
              model_sum = sum(vc_fh_count_trunc_infl)) %>% 
    head()
  head(my_spec$agg_comp_fh$agg_comp_data %>% select(PUMA, vulnerability_cat, obs_count_puma))
  head(my_spec$agg_comp_model$agg_comp_data %>% select(PUMA, vulnerability_cat, obs_count_puma))
}

```


#### Map Components of the SAE Estimates

```{r set up comparisons of data}
pov_p <- 
  lapply(c("05", "612"),
         function(a) {
           get(glue("sae_results_incpov_ctrls_mix_{a}"))$sae_out %>% 
             filter(vc_value %in% c("0%-50%", "50%-100%")) %>% 
             group_by(GEOID) %>% 
             summarize(acs1_pov       = sum(share_direct),
                       sae_pov_fh     = sum(share_fh_trunc),
                       sae_pov_model  = sum(share_model_trunc))
         }
  )

tractShp_pov <- 
  tractShp %>% 
  left_join(pov_p[[1]] %>% rename(acs1_pov_05      = acs1_pov,
                                  sae_pov_fh_05    = sae_pov_fh,
                                  sae_pov_model_05 = sae_pov_model),
            by = "GEOID") %>% 
  left_join(pov_p[[2]] %>% rename(acs1_pov_612      = acs1_pov,
                                  sae_pov_fh_612    = sae_pov_fh,
                                  sae_pov_model_612 = sae_pov_model),
            by = "GEOID") %>% 
  left_join(acs5tract %>% select(GEOID, acs5_pov = incpov_r0to100_est),
            by = "GEOID")

```

```{r set color scale}
map_sae_components <- function(dt, fill_field, field_label, val_limits, save_label) {
  
  pumas_to_keep <-
    geo_crosswalk[GEOID %in% dt$GEOID] %>% 
    .[["PUMA"]] %>% 
    unique()
  
  pumaShp_sub <-
    pumaShp %>% 
    filter(PUMA %in% pumas_to_keep)
  
  my_plot <- 
    ggplot() +
    geom_sf(data = dt,
            aes_string(fill = fill_field),
            linewidth = 0.025,
            color = "black") +
    geom_sf(data = pumaShp_sub,
            color = "red",
            linewidth = 0.75,
            fill = NA) +
    scale_fill_viridis_c(name   = field_label,
                         limits = val_limits,
                         option = "viridis", #"plasma", # 
                         trans  = "identity",
                         labels = percent,
                         alpha  = .4) +
    theme_void() +
    theme(legend.position = "none")
  
  ggsave(plot = my_plot,
         filename = glue("{output_path}Map of Base Year {save_label}_{my_output_tag}.png"),
         #width = 7,
         height = 7,
         units = "in")
  
  print(my_plot)
}
```


```{r set value scales}
limits_05  <- range(with(tractShp_pov, c(acs1_pov_05)),  # sae_pov_fh_05, sae_pov_model_05
                    na.rm = TRUE)
limits_612 <- range(with(tractShp_pov, c(acs1_pov_612)), # sae_pov_fh_612, sae_pov_model_612
                    na.rm = TRUE)
```

```{r generate maps}
# /!\ Look into why some geographies are displaying as "NA". Must of course do with underlying data
# In Chicago, O'Hare is a valid missing. However, others shouldn't be.
# /*\ It may be desirable to filter these maps to a sub-state geography because,
# at the state level, more dense areas are simultaneously often those with higher
# poverty values, and also areas that won't show up clearly on a state-wide map
my_dt <- tractShp_pov #%>% filter(str_detect(GEOID, "17031"))

map_sae_components(my_dt, "acs1_pov_05",      "", val_limits = limits_05, save_label = "Poverty (ACS1), age 0-5")
map_sae_components(my_dt, "acs5_pov",         "", val_limits = limits_05, save_label = "Poverty (ACS5), age 0-5")
map_sae_components(my_dt, "sae_pov_model_05", "", val_limits = limits_05, save_label = "Poverty (SAE), age 0-5")

```

```{r build comparison of selected SAE with ACS1, eval = developer_mode}
# Build 
(puma_agg_sae <- 
  sae_results_ccdf_wk_pov_05$sae_out %>% 
   as.data.table() %>%
  .[j = PUMA := as.numeric(PUMA)] %>% 
  .[str_detect(vc_value, "^WorkElig_(0%|75%|150%).+"),
    .(n_p = sum(n_pt),
      n_p_elig    = sum(vc_count),
      n_p_elig_fh    = sum(vc_fh_count_trunc),
      n_p_elig_model = sum(vc_model_count_trunc)),
    by = PUMA] %>% 
  .[j = `:=`(pct_elig       = n_p_elig       / n_p,
             pct_elig_fh    = n_p_elig_fh    / n_p,
             pct_elig_model = n_p_elig_model / n_p)] %>% 
   arrange(PUMA) %>% 
  select(PUMA, n_sae = n_p, 
         n_ccap_elig_fh = n_p_elig_fh, 
         n_ccap_elig_model = n_p_elig_model,
         pct_ccap_elig       = pct_elig,
         pct_ccap_elig_fh    = pct_elig_fh, 
         pct_ccap_elig_model = pct_elig_model))
  # aggregate counts across vc_values and puma
  # compare with puma aggregate calcs

(puma_agg_acs1 <- 
   acs1_child[between(AGEP, 0, 5),
              .(sample_n = .N,
                n_acs1 = sum(PWGTP),
                  n_lt225fpl         = sum(PWGTP[fam_incpov_ratio <= 2.25]),
                pct_lt225fpl         = sum(PWGTP[fam_incpov_ratio <= 2.25]) / sum(PWGTP),
                  n_lt225fpl_worksch = sum(PWGTP[fam_incpov_ratio <= 2.25 & all_work_sch == 1]),
                pct_lt225fpl_worksch = sum(PWGTP[fam_incpov_ratio <= 2.25 & all_work_sch == 1]) / sum(PWGTP)),
              by = .(PUMA = as.numeric(PUMA))] %>% 
   arrange(PUMA))

puma_agg_comp <- 
  merge(puma_agg_acs1,
        puma_agg_sae,
        by = "PUMA")
```


```{r visualize comparison of selected SAE with ACS1, eval = developer_mode}
# Compare counts
plot_puma_aggs_ns <- 
  ggplot(puma_agg_comp,
       aes(x = n_lt225fpl_worksch,
           group = PUMA)) +
  geom_point(aes(y = n_ccap_elig_fh),
             color = "blue") +
  geom_point(aes(y = n_ccap_elig_model),
             color = "green") +
  geom_point(data = puma_agg_comp %>% filter(PUMA == 02100),
             aes(y = n_ccap_elig_fh),
             color = "orange") +
  geom_point(data = puma_agg_comp %>% filter(PUMA == 02100),
             aes(y = n_ccap_elig_model),
             color = "red") +
  geom_abline() +
  scale_x_continuous(limits = c(0, NA)) +
  scale_y_continuous(limits = c(0, NA)) +
  theme_minimal()

ggplotly(plot_puma_aggs_ns, tooltip = "group")

# Compare shares
plot_puma_aggs_pcts <- 
  ggplot(puma_agg_comp,
       aes(x = pct_lt225fpl_worksch,
           group = PUMA)) +
  geom_point(aes(y = pct_ccap_elig),
             color = "black") +
  geom_point(aes(y = pct_ccap_elig_fh),
             color = "blue") +
  geom_point(aes(y = pct_ccap_elig_model),
             color = "green") +
  geom_point(data = puma_agg_comp %>% filter(PUMA == 02100),
             aes(y = pct_ccap_elig_fh),
             color = "orange") +
  geom_point(data = puma_agg_comp %>% filter(PUMA == 02100),
             aes(y = pct_ccap_elig_model),
             color = "red") +
  geom_abline() +
  scale_x_continuous(limits = c(0, NA)) +
  scale_y_continuous(limits = c(0, NA)) +
  theme_minimal()

ggplotly(plot_puma_aggs_pcts, tooltip = "group")

```


```{r check consistency of base population measures, eval = developer_mode}
# /!\ Consider moving this to 01e if it's interesting

puma_agg_pop <- 
  pop_by_age %>% 
  rename(GEOID = GEOID_TR20) %>% 
  merge(geo_crosswalk[j = .(GEOID, PUMA = as.numeric(PUMA))] %>% unique(),
        by = "GEOID") %>% 
  as.data.table() %>% 
  .[year == base_year,
    j = .(n_pop = sum(age_0to5_count)),
    by = PUMA] %>% 
  arrange(PUMA)

puma_agg_comp_pop <- 
  puma_agg_comp %>% 
  merge(puma_agg_pop,
        by = "PUMA")

plot_puma_pops <- 
  ggplot(puma_agg_comp_pop,
         aes(x = n_acs1,
             y = n_pop,
             group = PUMA)) +
  geom_point() + 
  geom_point(data = puma_agg_comp_pop %>% filter(PUMA == "02100"),
             color = "red") + 
  geom_abline() +
  scale_x_continuous(limits = c(0, NA)) +
  scale_y_continuous(limits = c(0, NA)) +
  theme_minimal()


ggplotly(plot_puma_pops, tooltip = "group")
```

```{r investigate the amount of noise in the ACS1 sample}
# The counts of young children (age 0-5) are very low
acs1_child[between(AGEP, 0, 5), 
           .N, 
           by = PUMA] %>% 
  .[order(PUMA)] %>% 
  ggplot(aes(x = N)) + 
  geom_histogram() +
  labs(title = "")

# Bootstrapping values for each PUMA
```

